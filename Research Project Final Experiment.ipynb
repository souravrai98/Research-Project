{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2ea57c",
   "metadata": {},
   "source": [
    "## Implementing Resnet from Scratch to Add custom number of layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc4be2a",
   "metadata": {},
   "source": [
    "Resnet has two main components BasicBlock and make layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c3de29",
   "metadata": {},
   "source": [
    "### Basic Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "007d305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from typing import Type\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c90658",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes, filters, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.First_Conv = nn.Conv2d(inplanes, filters, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "        self.Batch_Norm_1 = nn.BatchNorm2d(filters)\n",
    "        self.Relu1 = nn.ReLU(inplace=True)\n",
    "        self.Second_Conv = nn.Conv2d(filters, filters, kernel_size=3, stride=1,\n",
    "                     padding=1, bias=False)\n",
    "        self.Batch_Norm_2 = nn.BatchNorm2d(filters)\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Normal addition is replaced with Skip addition for quantization purposes\n",
    "        self.skip_addition = nn.quantized.FloatFunctional()\n",
    "        \n",
    "        self.Relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        # 2 Convolutional layers are stacked together\n",
    "        \n",
    "        z = self.First_Conv(x)\n",
    "        z = self.Batch_Norm_1(z)\n",
    "        z = self.Relu1(z)\n",
    "\n",
    "        z = self.Second_Conv(z)\n",
    "        z = self.Batch_Norm_2(z)\n",
    "\n",
    "        # Downsample the input if the size of the input and the output is different\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        #Skip connection (output = F(x) + x) \n",
    "        \n",
    "        z = self.skip_addition.add(identity, z)\n",
    "        z = self.Relu2(z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165781a6",
   "metadata": {},
   "source": [
    "### Make Layer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912bab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layers(block_type, inplanes, filters, num_blocks, stride =1):\n",
    "    downsample = None\n",
    "    \n",
    "    if stride != 1 or inplanes != filters:\n",
    "        downsample = nn.Sequential(\n",
    "        nn.Conv2d(inplanes, filters, 1, stride, bias=False),\n",
    "        nn.BatchNorm2s(filters),\n",
    "        )\n",
    "        \n",
    "    layers_list = []\n",
    "    layers_list.append(block_type(inplanes,filters, stride, downsample))\n",
    "    inplanes = filters\n",
    "    for _ in range(1, blocks):\n",
    "        layers.append(block_type(inplanes,filters))\n",
    "    return nn.Sequential(*layers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fadfb05",
   "metadata": {},
   "source": [
    "We finally implement the Custom Resnet Class using the components defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd628b",
   "metadata": {},
   "source": [
    "### Custom Resnet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd574f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetCustom(nn.Module):\n",
    "    def __init__(self, block_type , layers_list, num_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.inplanes = 64\n",
    "        \n",
    "        self.First_Conv = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.Batch_Norm_1 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.Relu = nn.ReLU(inplace=True)\n",
    "        self.MaxPool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.Layer_1 = self.make_layers(block_type, 64, layers_list[0])\n",
    "        self.Layer_2 = self.make_layers(block_type, 128, layers_list[1], stride=2)\n",
    "        self.Layer_3 = self.make_layers(block_type, 256, layers_list[2], stride=2)\n",
    "        self.Layer_4 = self.make_layers(block_type, 512, layers_list[3], stride=2)\n",
    "\n",
    "        self.AvgPool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.Fully_Connected_1 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def make_layers(self, block_type, filters, num_blocks, stride =1):\n",
    "        downsample = None\n",
    "    \n",
    "        if stride != 1 or self.inplanes != filters:\n",
    "            downsample = nn.Sequential(\n",
    "            nn.Conv2d(self.inplanes, filters, 1, stride, bias=False),\n",
    "            nn.BatchNorm2d(filters),\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block_type(self.inplanes,filters, stride, downsample))\n",
    "        self.inplanes = filters\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block_type(self.inplanes,filters))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First Conv Layer\n",
    "        \n",
    "        z = self.First_Conv(x)\n",
    "        z = self.Batch_Norm_1(z)\n",
    "        z = self.Relu(z)\n",
    "        z = self.MaxPool(z)\n",
    "        \n",
    "        # Collection of Basic Blocks which implements skip connection and each basic block has 2 convolutional layers\n",
    "        \n",
    "        z = self.Layer_1(z)          \n",
    "        z = self.Layer_2(z)         \n",
    "        z = self.Layer_3(z)         \n",
    "        z = self.Layer_4(z)\n",
    "    \n",
    "        # Here we have an average pooling layer + flattening + fully connected layer\n",
    "        \n",
    "        z = self.AvgPool(z)         \n",
    "        z = torch.flatten(z, 1)     \n",
    "        z = self.Fully_Connected_1(z)\n",
    "        \n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f8c35",
   "metadata": {},
   "source": [
    "Passing a Random Tensor to our Custom Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580f9055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetCustom(\n",
      "  (First_Conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (Batch_Norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (Relu): ReLU(inplace=True)\n",
      "  (MaxPool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (Layer_1): Sequential(\n",
      "    (0): CustomBasicBlock(\n",
      "      (First_Conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (Relu1): ReLU(inplace=True)\n",
      "      (Second_Conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_addition): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (Relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): CustomBasicBlock(\n",
      "      (First_Conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (Relu1): ReLU(inplace=True)\n",
      "      (Second_Conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_addition): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (Relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Layer_2): Sequential(\n",
      "    (0): CustomBasicBlock(\n",
      "      (First_Conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (Relu1): ReLU(inplace=True)\n",
      "      (Second_Conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_addition): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (Relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): CustomBasicBlock(\n",
      "      (First_Conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (Relu1): ReLU(inplace=True)\n",
      "      (Second_Conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_addition): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (Relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Layer_3): Sequential(\n",
      "    (0): CustomBasicBlock(\n",
      "      (First_Conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (Relu1): ReLU(inplace=True)\n",
      "      (Second_Conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_addition): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (Relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): CustomBasicBlock(\n",
      "      (First_Conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (Relu1): ReLU(inplace=True)\n",
      "      (Second_Conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_addition): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (Relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): CustomBasicBlock(\n",
      "      (First_Conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (Relu1): ReLU(inplace=True)\n",
      "      (Second_Conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_addition): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (Relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): CustomBasicBlock(\n",
      "      (First_Conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (Relu1): ReLU(inplace=True)\n",
      "      (Second_Conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_addition): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (Relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Layer_4): Sequential(\n",
      "    (0): CustomBasicBlock(\n",
      "      (First_Conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (Relu1): ReLU(inplace=True)\n",
      "      (Second_Conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_addition): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (Relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): CustomBasicBlock(\n",
      "      (First_Conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (Relu1): ReLU(inplace=True)\n",
      "      (Second_Conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (Batch_Norm_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_addition): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (Relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (AvgPool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (Fully_Connected_1): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "13,542,986 total parameters.\n",
      "13,542,986 training parameters.\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand([1, 3, 224, 224])\n",
    "layers=[2, 2, 4, 2]\n",
    "model = ResNetCustom(CustomBasicBlock, layers, 10)\n",
    "print(model)\n",
    "    \n",
    "    # Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "output = model(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a345a8a",
   "metadata": {},
   "source": [
    "### Resnet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f04ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[2, 2, 2, 2]\n",
    "resnet18 = ResNetCustom(CustomBasicBlock, layers, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94944c9e",
   "metadata": {},
   "source": [
    "### Resnet 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4362f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[2, 2, 4, 2]\n",
    "resnet22 = ResNetCustom(CustomBasicBlock, layers, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c1b3d",
   "metadata": {},
   "source": [
    "### Resnet 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f99705",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[2, 4, 4, 2]\n",
    "resnet26 = ResNetCustom(CustomBasicBlock, layers, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d282b0",
   "metadata": {},
   "source": [
    "### Resnet 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b98b061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[3, 4, 4, 3]\n",
    "resnet30 = ResNetCustom(CustomBasicBlock, layers, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd64226",
   "metadata": {},
   "source": [
    "### Resnet 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b31e4216",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[3, 4, 6, 3]\n",
    "resnet34 = ResNetCustom(CustomBasicBlock, layers, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567ffa4",
   "metadata": {},
   "source": [
    "### Setting up the dataloader for the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60dab129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=256, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f3fa2",
   "metadata": {},
   "source": [
    "### Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dabf4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet22\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a21f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr = 0.01,momentum = 0.9, weight_decay = 5e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e95fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfc6b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec70b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "        Run one train epoch\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda()\n",
    "        input_var = input.cuda()\n",
    "        target_var = target\n",
    "        \n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee2103",
   "metadata": {},
   "source": [
    "### Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a218147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, device):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.to(device)\n",
    "            input_var = input.to(device)\n",
    "            target_var = target.to(device)\n",
    "\n",
    "            \n",
    "\n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output.data, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                          i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                          top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'\n",
    "          .format(top1=top1))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a81b7baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-02\n",
      "Epoch: [0][0/196]\tTime 1.365 (1.365)\tData 0.146 (0.146)\tLoss 2.4358 (2.4358)\tPrec@1 14.844 (14.844)\n",
      "Epoch: [0][50/196]\tTime 0.054 (0.061)\tData 0.032 (0.017)\tLoss 1.7824 (1.9029)\tPrec@1 39.062 (30.653)\n",
      "Epoch: [0][100/196]\tTime 0.019 (0.049)\tData 0.001 (0.018)\tLoss 1.4561 (1.7797)\tPrec@1 43.359 (34.704)\n",
      "Epoch: [0][150/196]\tTime 0.025 (0.045)\tData 0.001 (0.017)\tLoss 1.5983 (1.7025)\tPrec@1 42.578 (37.779)\n",
      "Test: [0/100]\tTime 0.118 (0.118)\tLoss 1.4243 (1.4243)\tPrec@1 51.000 (51.000)\n",
      "Test: [50/100]\tTime 0.005 (0.012)\tLoss 1.5195 (1.5123)\tPrec@1 50.000 (47.922)\n",
      " * Prec@1 47.260\n",
      "current lr 9.99753e-03\n",
      "Epoch: [1][0/196]\tTime 0.179 (0.179)\tData 0.152 (0.152)\tLoss 1.4110 (1.4110)\tPrec@1 50.391 (50.391)\n",
      "Epoch: [1][50/196]\tTime 0.021 (0.036)\tData 0.002 (0.015)\tLoss 1.2553 (1.3732)\tPrec@1 56.250 (49.602)\n",
      "Epoch: [1][100/196]\tTime 0.019 (0.037)\tData 0.001 (0.017)\tLoss 1.1820 (1.3346)\tPrec@1 61.719 (51.245)\n",
      "Epoch: [1][150/196]\tTime 0.019 (0.038)\tData 0.001 (0.018)\tLoss 1.1013 (1.3035)\tPrec@1 62.891 (52.520)\n",
      "Test: [0/100]\tTime 0.105 (0.105)\tLoss 1.2038 (1.2038)\tPrec@1 57.000 (57.000)\n",
      "Test: [50/100]\tTime 0.006 (0.012)\tLoss 1.2251 (1.2469)\tPrec@1 59.000 (55.549)\n",
      " * Prec@1 55.100\n",
      "current lr 9.99013e-03\n",
      "Epoch: [2][0/196]\tTime 0.181 (0.181)\tData 0.161 (0.161)\tLoss 1.1803 (1.1803)\tPrec@1 58.203 (58.203)\n",
      "Epoch: [2][50/196]\tTime 0.057 (0.039)\tData 0.034 (0.018)\tLoss 1.1297 (1.1538)\tPrec@1 54.297 (58.264)\n",
      "Epoch: [2][100/196]\tTime 0.059 (0.039)\tData 0.040 (0.019)\tLoss 0.9976 (1.1387)\tPrec@1 64.062 (58.938)\n",
      "Epoch: [2][150/196]\tTime 0.060 (0.039)\tData 0.038 (0.019)\tLoss 1.1847 (1.1213)\tPrec@1 57.031 (59.709)\n",
      "Test: [0/100]\tTime 0.098 (0.098)\tLoss 1.0884 (1.0884)\tPrec@1 62.000 (62.000)\n",
      "Test: [50/100]\tTime 0.005 (0.012)\tLoss 1.0026 (1.0980)\tPrec@1 66.000 (60.941)\n",
      " * Prec@1 61.140\n",
      "current lr 9.97781e-03\n",
      "Epoch: [3][0/196]\tTime 0.191 (0.191)\tData 0.165 (0.165)\tLoss 0.9864 (0.9864)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [3][50/196]\tTime 0.063 (0.040)\tData 0.042 (0.020)\tLoss 0.9030 (1.0203)\tPrec@1 67.578 (63.703)\n",
      "Epoch: [3][100/196]\tTime 0.024 (0.038)\tData 0.003 (0.018)\tLoss 1.0298 (1.0024)\tPrec@1 64.062 (64.213)\n",
      "Epoch: [3][150/196]\tTime 0.019 (0.037)\tData 0.001 (0.017)\tLoss 0.9327 (0.9919)\tPrec@1 67.578 (64.727)\n",
      "Test: [0/100]\tTime 0.106 (0.106)\tLoss 0.9148 (0.9148)\tPrec@1 73.000 (73.000)\n",
      "Test: [50/100]\tTime 0.006 (0.011)\tLoss 0.8453 (0.9925)\tPrec@1 74.000 (66.137)\n",
      " * Prec@1 66.410\n",
      "current lr 9.96057e-03\n",
      "Epoch: [4][0/196]\tTime 0.183 (0.183)\tData 0.158 (0.158)\tLoss 0.8512 (0.8512)\tPrec@1 69.141 (69.141)\n",
      "Epoch: [4][50/196]\tTime 0.051 (0.039)\tData 0.029 (0.019)\tLoss 0.8936 (0.9319)\tPrec@1 68.359 (67.042)\n",
      "Epoch: [4][100/196]\tTime 0.048 (0.037)\tData 0.029 (0.017)\tLoss 0.8874 (0.9171)\tPrec@1 68.359 (67.319)\n",
      "Epoch: [4][150/196]\tTime 0.026 (0.036)\tData 0.008 (0.016)\tLoss 0.8812 (0.9070)\tPrec@1 66.406 (67.917)\n",
      "Test: [0/100]\tTime 0.102 (0.102)\tLoss 0.8541 (0.8541)\tPrec@1 72.000 (72.000)\n",
      "Test: [50/100]\tTime 0.013 (0.011)\tLoss 0.7414 (0.8618)\tPrec@1 77.000 (70.098)\n",
      " * Prec@1 70.240\n",
      "current lr 9.93844e-03\n",
      "Epoch: [5][0/196]\tTime 0.184 (0.184)\tData 0.158 (0.158)\tLoss 0.9062 (0.9062)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [5][50/196]\tTime 0.030 (0.037)\tData 0.012 (0.017)\tLoss 0.9311 (0.8750)\tPrec@1 68.750 (68.842)\n",
      "Epoch: [5][100/196]\tTime 0.064 (0.038)\tData 0.043 (0.018)\tLoss 0.7604 (0.8611)\tPrec@1 70.312 (69.462)\n",
      "Epoch: [5][150/196]\tTime 0.032 (0.037)\tData 0.012 (0.017)\tLoss 0.9553 (0.8589)\tPrec@1 68.750 (69.611)\n",
      "Test: [0/100]\tTime 0.098 (0.098)\tLoss 0.8701 (0.8701)\tPrec@1 70.000 (70.000)\n",
      "Test: [50/100]\tTime 0.005 (0.011)\tLoss 0.8213 (0.8678)\tPrec@1 74.000 (70.569)\n",
      " * Prec@1 69.820\n",
      "current lr 9.91144e-03\n",
      "Epoch: [6][0/196]\tTime 0.185 (0.185)\tData 0.158 (0.158)\tLoss 0.8453 (0.8453)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [6][50/196]\tTime 0.054 (0.038)\tData 0.032 (0.018)\tLoss 0.7971 (0.7962)\tPrec@1 69.141 (71.867)\n",
      "Epoch: [6][100/196]\tTime 0.056 (0.037)\tData 0.034 (0.017)\tLoss 0.8233 (0.7883)\tPrec@1 69.531 (72.173)\n",
      "Epoch: [6][150/196]\tTime 0.050 (0.036)\tData 0.031 (0.017)\tLoss 0.6863 (0.7870)\tPrec@1 78.516 (72.281)\n",
      "Test: [0/100]\tTime 0.097 (0.097)\tLoss 0.8701 (0.8701)\tPrec@1 72.000 (72.000)\n",
      "Test: [50/100]\tTime 0.009 (0.011)\tLoss 0.7847 (0.8354)\tPrec@1 70.000 (72.137)\n",
      " * Prec@1 71.840\n",
      "current lr 9.87958e-03\n",
      "Epoch: [7][0/196]\tTime 0.184 (0.184)\tData 0.160 (0.160)\tLoss 0.7816 (0.7816)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [7][50/196]\tTime 0.019 (0.039)\tData 0.001 (0.018)\tLoss 0.8039 (0.7593)\tPrec@1 71.094 (73.606)\n",
      "Epoch: [7][100/196]\tTime 0.018 (0.040)\tData 0.000 (0.019)\tLoss 0.7990 (0.7571)\tPrec@1 73.828 (73.356)\n",
      "Epoch: [7][150/196]\tTime 0.020 (0.039)\tData 0.001 (0.019)\tLoss 0.6448 (0.7501)\tPrec@1 79.688 (73.756)\n",
      "Test: [0/100]\tTime 0.100 (0.100)\tLoss 0.8244 (0.8244)\tPrec@1 73.000 (73.000)\n",
      "Test: [50/100]\tTime 0.013 (0.011)\tLoss 0.7709 (0.8289)\tPrec@1 71.000 (71.902)\n",
      " * Prec@1 71.940\n",
      "current lr 9.84292e-03\n",
      "Epoch: [8][0/196]\tTime 0.205 (0.205)\tData 0.178 (0.178)\tLoss 0.6895 (0.6895)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [8][50/196]\tTime 0.049 (0.037)\tData 0.027 (0.018)\tLoss 0.7467 (0.7151)\tPrec@1 74.609 (74.648)\n",
      "Epoch: [8][100/196]\tTime 0.061 (0.039)\tData 0.040 (0.019)\tLoss 0.6975 (0.7162)\tPrec@1 76.953 (74.625)\n",
      "Epoch: [8][150/196]\tTime 0.059 (0.039)\tData 0.037 (0.019)\tLoss 0.7521 (0.7100)\tPrec@1 73.828 (75.041)\n",
      "Test: [0/100]\tTime 0.099 (0.099)\tLoss 0.7089 (0.7089)\tPrec@1 77.000 (77.000)\n",
      "Test: [50/100]\tTime 0.005 (0.011)\tLoss 0.6926 (0.7222)\tPrec@1 80.000 (75.314)\n",
      " * Prec@1 75.540\n",
      "current lr 9.80147e-03\n",
      "Epoch: [9][0/196]\tTime 0.185 (0.185)\tData 0.160 (0.160)\tLoss 0.7899 (0.7899)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [9][50/196]\tTime 0.064 (0.042)\tData 0.042 (0.022)\tLoss 0.7378 (0.6926)\tPrec@1 73.047 (75.965)\n",
      "Epoch: [9][100/196]\tTime 0.055 (0.041)\tData 0.034 (0.021)\tLoss 0.5877 (0.6908)\tPrec@1 80.469 (75.932)\n",
      "Epoch: [9][150/196]\tTime 0.061 (0.040)\tData 0.039 (0.020)\tLoss 0.5797 (0.6844)\tPrec@1 80.859 (76.123)\n",
      "Test: [0/100]\tTime 0.110 (0.110)\tLoss 0.7609 (0.7609)\tPrec@1 73.000 (73.000)\n",
      "Test: [50/100]\tTime 0.013 (0.012)\tLoss 0.6579 (0.7720)\tPrec@1 77.000 (73.725)\n",
      " * Prec@1 73.690\n"
     ]
    }
   ],
   "source": [
    "print_freq = 50\n",
    "best_prec1 = 0\n",
    "for epoch in range(0, 10):\n",
    "\n",
    "        # train for one epoch\n",
    "        print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
    "        train(trainloader, model, criterion, optimizer, epoch)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(testloader, model, criterion, device)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6098b216",
   "metadata": {},
   "source": [
    "### Inserting qunatization operators in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2ef601cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelQuantization(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(ModelQuantization,self).__init__()\n",
    "        \n",
    "        # Inserting a quantization operator before the input\n",
    "        self.quantstub = torch.quantization.QuantStub()\n",
    "        \n",
    "        # Inserting a dequantization operator after the output\n",
    "        self.dequantstub = torch.quantization.DeQuantStub()\n",
    "        \n",
    "        # Original floating point model\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Floating values to integer values\n",
    "        z = self.quantstub(x)\n",
    "        \n",
    "        z = self.model(z)\n",
    "        \n",
    "        # Integer values to floating values\n",
    "        z = self.dequantstub(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b56e9d9",
   "metadata": {},
   "source": [
    "### Fusing the Conv + Batch Norm Layers for correct quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd865527",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_dev = torch.device('cpu:0')\n",
    "model.to(cpu_dev)\n",
    "# Deep copy of the model for layer fusion\n",
    "fused_model = copy.deepcopy(model)\n",
    "\n",
    "model.eval()\n",
    "# Swithcing model in evaluation mode before layer fusion\n",
    "fused_model.eval()\n",
    "fused_model = torch.quantization.fuse_modules(fused_model, [[\"First_Conv\", \"Batch_Norm_1\", \"Relu\"]], inplace=True)\n",
    "for module_name, module in fused_model.named_children():\n",
    "        if \"layer\" in module_name:\n",
    "            for basic_block_name, basic_block in module.named_children():\n",
    "                torch.quantization.fuse_modules(basic_block, [[\"First_Conv\", \"Batch_Norm_1\", \"Relu1\"], [\"Second_Conv\", \"Batch_Norm_2\"]], inplace=True)\n",
    "                for sub_block_name, sub_block in basic_block.named_children():\n",
    "                    if sub_block_name == \"downsample\":\n",
    "                        torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68a7f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
    "\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model_1(x).detach().cpu().numpy()\n",
    "        y2 = model_2(x).detach().cpu().numpy()\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            return False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a68c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_dev, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5cc0f",
   "metadata": {},
   "source": [
    "### Setting up the model for quantization by inserting quantization operators and model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79b5c613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelQuantization(\n",
       "  (quantstub): QuantStub(\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequantstub): DeQuantStub()\n",
       "  (model): ResNetCustom(\n",
       "    (First_Conv): ConvReLU2d(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (Batch_Norm_1): Identity()\n",
       "    (Relu): Identity()\n",
       "    (MaxPool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (Layer_1): Sequential(\n",
       "      (0): CustomBasicBlock(\n",
       "        (First_Conv): Conv2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_1): BatchNorm2d(\n",
       "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu1): ReLU(inplace=True)\n",
       "        (Second_Conv): Conv2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_2): BatchNorm2d(\n",
       "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (skip_addition): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): CustomBasicBlock(\n",
       "        (First_Conv): Conv2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_1): BatchNorm2d(\n",
       "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu1): ReLU(inplace=True)\n",
       "        (Second_Conv): Conv2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_2): BatchNorm2d(\n",
       "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (skip_addition): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (Layer_2): Sequential(\n",
       "      (0): CustomBasicBlock(\n",
       "        (First_Conv): Conv2d(\n",
       "          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_1): BatchNorm2d(\n",
       "          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu1): ReLU(inplace=True)\n",
       "        (Second_Conv): Conv2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_2): BatchNorm2d(\n",
       "          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(\n",
       "            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (1): BatchNorm2d(\n",
       "            128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_addition): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): CustomBasicBlock(\n",
       "        (First_Conv): Conv2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_1): BatchNorm2d(\n",
       "          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu1): ReLU(inplace=True)\n",
       "        (Second_Conv): Conv2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_2): BatchNorm2d(\n",
       "          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (skip_addition): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (Layer_3): Sequential(\n",
       "      (0): CustomBasicBlock(\n",
       "        (First_Conv): Conv2d(\n",
       "          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_1): BatchNorm2d(\n",
       "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu1): ReLU(inplace=True)\n",
       "        (Second_Conv): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_2): BatchNorm2d(\n",
       "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(\n",
       "            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (1): BatchNorm2d(\n",
       "            256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_addition): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): CustomBasicBlock(\n",
       "        (First_Conv): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_1): BatchNorm2d(\n",
       "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu1): ReLU(inplace=True)\n",
       "        (Second_Conv): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_2): BatchNorm2d(\n",
       "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (skip_addition): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): CustomBasicBlock(\n",
       "        (First_Conv): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_1): BatchNorm2d(\n",
       "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu1): ReLU(inplace=True)\n",
       "        (Second_Conv): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_2): BatchNorm2d(\n",
       "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (skip_addition): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): CustomBasicBlock(\n",
       "        (First_Conv): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_1): BatchNorm2d(\n",
       "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu1): ReLU(inplace=True)\n",
       "        (Second_Conv): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_2): BatchNorm2d(\n",
       "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (skip_addition): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (Layer_4): Sequential(\n",
       "      (0): CustomBasicBlock(\n",
       "        (First_Conv): Conv2d(\n",
       "          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_1): BatchNorm2d(\n",
       "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu1): ReLU(inplace=True)\n",
       "        (Second_Conv): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_2): BatchNorm2d(\n",
       "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (1): BatchNorm2d(\n",
       "            512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "            (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (skip_addition): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): CustomBasicBlock(\n",
       "        (First_Conv): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_1): BatchNorm2d(\n",
       "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu1): ReLU(inplace=True)\n",
       "        (Second_Conv): Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Batch_Norm_2): BatchNorm2d(\n",
       "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (skip_addition): FloatFunctional(\n",
       "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (Relu2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (AvgPool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (Fully_Connected_1): Linear(\n",
       "      in_features=512, out_features=10, bias=True\n",
       "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserting qunatization operators on the fused model\n",
    "quantized_model = ModelQuantization(model=fused_model)\n",
    "  \n",
    "# Selecting quantization schemes on the model\n",
    "backend = 'fbgemm' # This backend is used when we want to quantize the model to work on Android devices\n",
    "#backend = 'fbgemm' # This backend is used to convert for X86 devices\n",
    "quantization_config = torch.quantization.get_default_qconfig(backend)\n",
    "   \n",
    "quantized_model.qconfig = quantization_config\n",
    "    \n",
    "# Print quantization configurations\n",
    "print(quantized_model.qconfig)\n",
    "\n",
    "# Preparing model for calibration\n",
    "torch.quantization.prepare(quantized_model, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ad8b5",
   "metadata": {},
   "source": [
    "### Calibration of the values of the quantization equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bfb2a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_calibration(model, data, device):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for input_values, target_values in data:\n",
    "        input_values,target_values = input_values.to(device), target_values.to(device)\n",
    "        \n",
    "        _ = model(input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eba7d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_calibration(quantized_model, trainloader, cpu_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af245038",
   "metadata": {},
   "source": [
    "### Finally we quantize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d827a11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelQuantization(\n",
      "  (quantstub): Quantize(scale=tensor([0.0408]), zero_point=tensor([60]), dtype=torch.quint8)\n",
      "  (dequantstub): DeQuantize()\n",
      "  (model): ResNetCustom(\n",
      "    (First_Conv): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.0534542053937912, zero_point=0, padding=(3, 3))\n",
      "    (Batch_Norm_1): Identity()\n",
      "    (Relu): Identity()\n",
      "    (MaxPool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (Layer_1): Sequential(\n",
      "      (0): CustomBasicBlock(\n",
      "        (First_Conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.10369772464036942, zero_point=73, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (Relu1): ReLU(inplace=True)\n",
      "        (Second_Conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.052992481738328934, zero_point=72, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (skip_addition): QFunctional(\n",
      "          scale=0.11102098226547241, zero_point=42\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (Relu2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): CustomBasicBlock(\n",
      "        (First_Conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.11682090908288956, zero_point=69, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (Relu1): ReLU(inplace=True)\n",
      "        (Second_Conv): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.042456578463315964, zero_point=66, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (skip_addition): QFunctional(\n",
      "          scale=0.1256941258907318, zero_point=40\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (Relu2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Layer_2): Sequential(\n",
      "      (0): CustomBasicBlock(\n",
      "        (First_Conv): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.11157062649726868, zero_point=63, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (Relu1): ReLU(inplace=True)\n",
      "        (Second_Conv): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.043040208518505096, zero_point=63, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.07847800105810165, zero_point=62, bias=False)\n",
      "          (1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_addition): QFunctional(\n",
      "          scale=0.11496477574110031, zero_point=61\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (Relu2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): CustomBasicBlock(\n",
      "        (First_Conv): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.06297101080417633, zero_point=64, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (Relu1): ReLU(inplace=True)\n",
      "        (Second_Conv): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.04680924490094185, zero_point=63, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (skip_addition): QFunctional(\n",
      "          scale=0.11906171590089798, zero_point=44\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (Relu2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Layer_3): Sequential(\n",
      "      (0): CustomBasicBlock(\n",
      "        (First_Conv): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.07691933959722519, zero_point=56, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (Relu1): ReLU(inplace=True)\n",
      "        (Second_Conv): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.03555840626358986, zero_point=56, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.0636264979839325, zero_point=62, bias=False)\n",
      "          (1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_addition): QFunctional(\n",
      "          scale=0.11150820553302765, zero_point=56\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (Relu2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): CustomBasicBlock(\n",
      "        (First_Conv): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.04907933622598648, zero_point=60, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (Relu1): ReLU(inplace=True)\n",
      "        (Second_Conv): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.052541229873895645, zero_point=55, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (skip_addition): QFunctional(\n",
      "          scale=0.12566275894641876, zero_point=31\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (Relu2): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): CustomBasicBlock(\n",
      "        (First_Conv): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.06839106231927872, zero_point=59, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (Relu1): ReLU(inplace=True)\n",
      "        (Second_Conv): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.04409763962030411, zero_point=63, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (skip_addition): QFunctional(\n",
      "          scale=0.14775630831718445, zero_point=29\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (Relu2): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): CustomBasicBlock(\n",
      "        (First_Conv): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.08189737796783447, zero_point=60, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (Relu1): ReLU(inplace=True)\n",
      "        (Second_Conv): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.042024947702884674, zero_point=63, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (skip_addition): QFunctional(\n",
      "          scale=0.15849636495113373, zero_point=27\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (Relu2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (Layer_4): Sequential(\n",
      "      (0): CustomBasicBlock(\n",
      "        (First_Conv): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.10630496591329575, zero_point=61, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (Relu1): ReLU(inplace=True)\n",
      "        (Second_Conv): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.08514835685491562, zero_point=66, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.11943051964044571, zero_point=62, bias=False)\n",
      "          (1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (skip_addition): QFunctional(\n",
      "          scale=0.1900196373462677, zero_point=59\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (Relu2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): CustomBasicBlock(\n",
      "        (First_Conv): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.08090878278017044, zero_point=62, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (Relu1): ReLU(inplace=True)\n",
      "        (Second_Conv): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.14714357256889343, zero_point=69, padding=(1, 1), bias=False)\n",
      "        (Batch_Norm_2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (skip_addition): QFunctional(\n",
      "          scale=0.23713447153568268, zero_point=56\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (Relu2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (AvgPool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (Fully_Connected_1): QuantizedLinear(in_features=512, out_features=10, scale=0.23387223482131958, zero_point=49, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# We finally qunatize the model\n",
    "quant_model_final = torch.quantization.convert(quantized_model, inplace=True)\n",
    "\n",
    "    \n",
    "# We put the model in evaluation mode\n",
    "quant_model_final.eval()\n",
    "\n",
    "# Print quantized model.\n",
    "print(quant_model_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67daddf",
   "metadata": {},
   "source": [
    "### Evaluating the floating point model and quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93a6bde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/100]\tTime 0.250 (0.250)\tLoss 0.7609 (0.7609)\tPrec@1 73.000 (73.000)\n",
      "Test: [50/100]\tTime 0.027 (0.032)\tLoss 0.6579 (0.7720)\tPrec@1 77.000 (73.725)\n",
      " * Prec@1 73.690\n",
      "Test: [0/100]\tTime 0.192 (0.192)\tLoss 0.7480 (0.7480)\tPrec@1 73.000 (73.000)\n",
      "Test: [50/100]\tTime 0.013 (0.017)\tLoss 0.6548 (0.7713)\tPrec@1 76.000 (73.647)\n",
      " * Prec@1 73.590\n",
      "The original model accuracy is  73.69\n",
      "Quantized model accuracy is 73.59\n"
     ]
    }
   ],
   "source": [
    "floating_point_prec = validate(testloader, model, criterion, cpu_dev)\n",
    "integer_point_prec = validate(testloader, quant_model_final, criterion, cpu_dev)\n",
    "\n",
    "print(\"The original model accuracy is \", floating_point_prec)\n",
    "print(\"Quantized model accuracy is\", integer_point_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f484af",
   "metadata": {},
   "source": [
    "### Measure Inference Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d53f8ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_latency(model,\n",
    "                              device,\n",
    "                              input_size=(1, 3, 32, 32),\n",
    "                              num_samples=100,\n",
    "                              num_warmups=10):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand(size=input_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmups):\n",
    "            _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_samples):\n",
    "            _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_time_ave = elapsed_time / num_samples\n",
    "\n",
    "    return elapsed_time_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63a33316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "       aten::_slow_conv2d_forward        46.14%       3.722ms        49.00%       3.953ms     171.870us            23  \n",
      "          aten::native_batch_norm        13.97%       1.127ms        17.55%       1.416ms      59.000us            24  \n",
      "         aten::mkldnn_convolution         5.89%     475.000us         6.25%     504.000us     504.000us             1  \n",
      "                      aten::relu_         5.84%     471.000us         7.72%     623.000us      29.667us            21  \n",
      "                      aten::empty         4.71%     380.000us         4.71%     380.000us       1.979us           192  \n",
      "                aten::convolution         4.34%     350.000us        64.32%       5.189ms     216.208us            24  \n",
      "               aten::_convolution         3.86%     311.000us        59.99%       4.839ms     201.625us            24  \n",
      "     aten::_batch_norm_impl_index         3.31%     267.000us        19.87%       1.603ms      66.792us            24  \n",
      "                        aten::add         2.14%     173.000us         2.14%     173.000us      17.300us            10  \n",
      "                 aten::clamp_min_         1.88%     152.000us         1.88%     152.000us       7.238us            21  \n",
      "                     aten::conv2d         1.33%     107.000us        65.65%       5.296ms     220.667us            24  \n",
      "                 aten::empty_like         1.00%      81.000us         1.59%     128.000us       5.333us            24  \n",
      "    aten::max_pool2d_with_indices         0.94%      76.000us         0.94%      76.000us      76.000us             1  \n",
      "                aten::thnn_conv2d         0.87%      70.000us        49.87%       4.023ms     174.913us            23  \n",
      "                    aten::resize_         0.63%      51.000us         0.63%      51.000us       2.217us            23  \n",
      "                       aten::view         0.62%      50.000us         0.62%      50.000us       2.174us            23  \n",
      "                      aten::addmm         0.46%      37.000us         0.58%      47.000us      47.000us             1  \n",
      "                       aten::div_         0.30%      24.000us         0.67%      54.000us      54.000us             1  \n",
      "                       aten::mean         0.29%      23.000us         1.25%     101.000us     101.000us             1  \n",
      "                        aten::sum         0.25%      20.000us         0.30%      24.000us      24.000us             1  \n",
      "                aten::as_strided_         0.16%      13.000us         0.16%      13.000us      13.000us             1  \n",
      "                   aten::_to_copy         0.16%      13.000us         0.32%      26.000us      26.000us             1  \n",
      "                      aten::copy_         0.15%      12.000us         0.15%      12.000us       6.000us             2  \n",
      "                    aten::flatten         0.14%      11.000us         0.26%      21.000us      21.000us             1  \n",
      "                          aten::t         0.14%      11.000us         0.25%      20.000us      20.000us             1  \n",
      "             aten::_reshape_alias         0.12%      10.000us         0.12%      10.000us      10.000us             1  \n",
      "                 aten::max_pool2d         0.11%       9.000us         1.05%      85.000us      85.000us             1  \n",
      "                     aten::linear         0.11%       9.000us         0.94%      76.000us      76.000us             1  \n",
      "        aten::adaptive_avg_pool2d         0.07%       6.000us         1.33%     107.000us     107.000us             1  \n",
      "              aten::empty_strided         0.06%       5.000us         0.06%       5.000us       5.000us             1  \n",
      "                  aten::transpose         0.06%       5.000us         0.11%       9.000us       9.000us             1  \n",
      "                 aten::as_strided         0.06%       5.000us         0.06%       5.000us       2.500us             2  \n",
      "                     aten::expand         0.06%       5.000us         0.07%       6.000us       6.000us             1  \n",
      "                      aten::fill_         0.05%       4.000us         0.05%       4.000us       4.000us             1  \n",
      "                         aten::to         0.05%       4.000us         0.37%      30.000us      30.000us             1  \n",
      "          aten::_nnpack_available         0.01%       1.000us         0.01%       1.000us       0.043us            23  \n",
      "               aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                 aten::batch_norm        -0.29%     -23.000us        20.90%       1.686ms      70.250us            24  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 8.067ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-03-25 12:15:26 31746:31746 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-03-25 12:15:26 31746:31746 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-03-25 12:15:26 31746:31746 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "inpp = torch.rand(size=(1, 3, 32, 32))\n",
    "with torch.autograd.profiler.profile() as prof:\n",
    "    output = model(inpp)\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d41d9939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                quantized::conv2d        69.73%       6.350ms        70.93%       6.459ms     280.826us            23  \n",
      "          quantized::batch_norm2d         7.06%     643.000us        11.86%       1.080ms      46.957us            23  \n",
      "           quantized::conv2d_relu         4.55%     414.000us         5.67%     516.000us     516.000us             1  \n",
      "    aten::_empty_affine_quantized         3.33%     303.000us         3.33%     303.000us       4.967us            61  \n",
      "                   quantized::add         3.06%     279.000us         3.42%     311.000us      31.100us            10  \n",
      "                      aten::relu_         2.34%     213.000us         2.34%     213.000us      10.650us            20  \n",
      "                      aten::empty         1.91%     174.000us         1.91%     174.000us       3.625us            48  \n",
      "        aten::quantize_per_tensor         1.79%     163.000us         1.79%     163.000us     163.000us             1  \n",
      "                 aten::empty_like         1.44%     131.000us         3.31%     301.000us       6.543us            46  \n",
      "                       aten::item         1.18%     107.000us         1.60%     146.000us       3.042us            48  \n",
      "                      aten::clone         0.80%      73.000us         0.93%      85.000us      85.000us             1  \n",
      "                quantized::linear         0.63%      57.000us         0.71%      65.000us      65.000us             1  \n",
      "       aten::quantized_max_pool2d         0.56%      51.000us         0.72%      66.000us      66.000us             1  \n",
      "        aten::_local_scalar_dense         0.45%      41.000us         0.45%      41.000us       0.854us            48  \n",
      "       aten::_adaptive_avg_pool2d         0.26%      24.000us         0.29%      26.000us      26.000us             1  \n",
      "                 aten::dequantize         0.20%      18.000us         0.21%      19.000us      19.000us             1  \n",
      "                    aten::flatten         0.16%      15.000us         0.30%      27.000us      27.000us             1  \n",
      "             aten::_reshape_alias         0.13%      12.000us         0.13%      12.000us      12.000us             1  \n",
      "                 aten::contiguous         0.08%       7.000us         1.01%      92.000us      92.000us             1  \n",
      "                 aten::max_pool2d         0.08%       7.000us         0.80%      73.000us      73.000us             1  \n",
      "                    aten::q_scale         0.07%       6.000us         0.07%       6.000us       0.072us            83  \n",
      "        aten::adaptive_avg_pool2d         0.07%       6.000us         0.35%      32.000us      32.000us             1  \n",
      "                    aten::resize_         0.05%       5.000us         0.05%       5.000us       5.000us             1  \n",
      "                    aten::qscheme         0.04%       4.000us         0.04%       4.000us       0.121us            33  \n",
      "               aten::q_zero_point         0.03%       3.000us         0.03%       3.000us       0.029us           103  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 9.106ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-03-25 12:16:29 31746:31746 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-03-25 12:16:29 31746:31746 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-03-25 12:16:29 31746:31746 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "inpp = torch.rand(size=(1, 3, 32, 32))\n",
    "with torch.autograd.profiler.profile() as prof:\n",
    "    output = quant_model_final(inpp)\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9898cc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.464544773101807"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_inference_latency(quant_model_final, device = cpu_dev)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc8447d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8365817070007324"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_inference_latency(model, device = cpu_dev)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5733604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
